# 2019年12月

## 12月13日

金沢出張の書類出した。

査読、acceptの判断をした論文のreject判定のお知らせ。珍しい。というか、おそらく初めてじゃなかろうか？嘘はついていないし、非常に冗長で役に立つかと言われると微妙だけど、がんばって書いたとおぼしき論文なので、publishしても良いと思ったのだが・・・

## 12月11日

金沢の講義計画

12/18(水) 3,4限 (初日は午後から)
12/19(木) 2,3,4限
12/20(金) 2,3,4限

0. 本講義の概要と目的
1. 分子動力学法の理論的背景
2. 圧力と界面張力
3. 温度制御とエルゴード性
4. 分子動力学法における数値積分法
5. (スライド)特別講演会「スパコンでできること、できないこと」
6. (スライド) 分子動力学法の実装と基本的アルゴリズム
7. (スライド) 分子動力学法の高速化手法について
8. (スライド) 分子動力学法の並列化とプログラム設計

## 12月9日

プログラム基礎同演習の講義日程確認

* 第10回 12/10 「Pythonが動く仕組み」
* 第11回 12/17 「動的計画法」
* 第11回 12/24 「乱数を使ったプログラム」

今週中にここまでやればなんとかなる。

後で金沢の講義計画を練ること。

VS Code上でGit管理をすることにした。
そのためにGit Historyを入れた。

## 12月4日

AIに悪意はあるか

AIに意識があるか、という問題は難しい。個人的には「チューリングテストをパスしたAIには意識があるとみなす」という立場であり、究極的には人間の意識とAIの持つ「意識」は区別不可能となる未来が来るだろうと思っているが、現時点ではAIは発展途上であり、「あぁ、まだソフトウェアなんだな」と思う時と「人間と同じ問題を抱えているのでは」と思う時の両方がある。

「あ、AIはただのソフトウェアなんだな」と思う例の一つは画像認識である。「犬」「猫」「羊」などのラベルがついた画像を事前に学習させておくことで、写真に何が写っているかを認識するAIを作るものだ。一見するとこのAIは写真に写るものを正しく認識しているように見えるが、何もいない草原に「羊がいる」と判断してしまう。「羊」のラベルがついた画像のほとんどが草原であったため、「草原＝羊」と認識してしまったのだ。逆に、子供が牧場で羊を抱きかかえる写真を見て「犬」と判断してしまうこともあった。草原以外にいる羊のデータがなかったためだと思われる。同様な例に「ハスキーと狼問題」がある。シベリアン・ハスキーは狼に似た犬種であるが、そのハスキーと狼を見分けるAIを作ったところ、実は犬ではなく「背景に雪があるかどうか」で判断していることがあった。

多くの場合、こうした画像認識の失敗は笑い話で済むが、差別問題がからむとやっかいなことになる。2015年、Googleは、写真管理アプリGoogle Photosをリリースしたが、そのアプリには写真に写っているものを認識し、ラベル付けする機能があった。しかし、黒人女性が写る写真に「ゴリラ」とタグ付けしてしまい、GoogleのChief Social Architectが謝罪する事態となった。これについては「差別的な人間が黒人に『ゴリラ』という差別的なタグをつけていたものを学習したせいだ」という噂も流れたが、どうやら純粋に「白人以外」のデータが足りずに、素で間違ったようだ。他にも、やはり黒人女性を「猿(Apes)」と認識してしまうことがわかった。結局Googleは根本解決ができず、「ゴリラ、猿、チンパンジー」といったラベルを禁止ワードにすることで対処することになった。

Google翻訳がジェンダーバイアスを持つことも知られている。例えば現時点(2019年12月4日現在)では、「**医者** は旅行先でカバンを忘れてきた。」は「The doctor has forgotten **his** bag at the travel destination.」と訳すが、「**看護師** は旅行先で……」とすると「The nurse has forgotten **her** bag when traveling.」と訳す。Google翻訳は多くの翻訳例を通じて学習したモデルを用いているが、そのデータ上で「Docter」が「his」と、「Nurse」が「her」と一緒に用いらていれることが多かったのだと思われる。

Google Photosは「学習データに白人が多く黒人が少なかったため」に起きたことであり、Google翻訳は「学習用データを通じて医者は男性が多く、看護師は女性が多い」という「偏見」も一緒に学習してしまったために起きたことだ。現時点では「AI」に罪はなく、「AIの学習過程で人間持つの差別や偏見が注入された」と認識されているが、そもそも学習で偏った情報に触れて偏見を身につけてしまうという過程は人間と全く同じである。そのうちAIが高度に発展した場合、我々はAIそのものに悪意や偏見を感じるようになるのかもしれない。

* Notes on AI Bias[https://www.ben-evans.com/benedictevans/2019/4/15/notes-on-ai-bias](https://www.ben-evans.com/benedictevans/2019/4/15/notes-on-ai-bias)
